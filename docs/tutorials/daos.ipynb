{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9e30da",
   "metadata": {},
   "source": [
    "# DAOS Filesystem with Tensorflow (Using MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b37505",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to use read and write files on [DAOS Filesystem](https://docs.daos.io/) with TensorFlow, through TensorFlow IO's DAOS file system integration.\n",
    "\n",
    "A machine running DAOS natively or through a [docker emulator](https://github.com/daos-stack/daos/tree/master/utils/docker) is needed to run this tutorial and/or use the Tensorflow IO DAOS integration. The DAOS Pool and Container used for this tutorial will be created and deleted within this tutorial, where we will be training and testing a simple Neural Network on the MNIST Dataset loaded from the DAOS File System Plugin.\n",
    "\n",
    "The pool and container id or label are part of the filename uri:\n",
    "```\n",
    "dfs://<pool_id>/<cont_id>/<path>\n",
    "dfs://<pool-label>/cont-label/<path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef6a9",
   "metadata": {},
   "source": [
    "## Setup and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad41a1a",
   "metadata": {},
   "source": [
    "### Install required packages, and restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x \n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7de300",
   "metadata": {},
   "source": [
    "### Create Pool and Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79528fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dmg -i pool create -s 500M TEST_POOL\n",
    "!daos cont create --pool=TEST_POOL --type=POSIX TEST_CONT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing our dfs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_url = \"dfs://TEST_POOL/TEST_CONT/\" # This the path we'll be using to load and access the dataset\n",
    "pwd = !pwd\n",
    "posix_url = pwd[0] + \"/tests/test_dfs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/train.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/train_labels.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/test.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/test_labels.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the Data from the POSIX Filesystem to the DAOS Filesystem under the pool and container we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"train.gz\", \"test.gz\", \"train_labels.gz\", \"test_labels.gz\"]\n",
    "for file in file_names:\n",
    "  tf.io.gfile.copy(posix_url + file, dfs_url + file, True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Our Training Images and Training Labels Exist under the specified pool and container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dfs_url + \"train.gz\"\n",
    "labels = dfs_url + \"train_labels.gz\"\n",
    "if tf.io.gfile.exists(images) and tf.io.gfile.exists(labels):\n",
    "  print(\"True\")\n",
    "else:\n",
    "  print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading MNIST Data from the DFS using tensorflow-io's built in MNIST loading functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_train = tfio.IODataset.from_mnist(\n",
    "    images,\n",
    "    labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing and Building a simple Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the elements of the dataset.\n",
    "d_train = d_train.shuffle(buffer_size=1024)\n",
    "\n",
    "# By default image data is uint8, so convert to float32 using map().\n",
    "d_train = d_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
    "\n",
    "# prepare batches the data just like any other tf.data.Dataset\n",
    "d_train = d_train.batch(32)\n",
    "\n",
    "# Build the model.\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, training on the dataset for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(d_train, epochs=15, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of Loss vs Epoch during Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Test Data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = dfs_url + \"test.gz\"\n",
    "test_labels = dfs_url + \"test_labels.gz\"\n",
    "if tf.io.gfile.exists(test_images) and tf.io.gfile.exists(test_labels):\n",
    "  print(\"True\")\n",
    "else:\n",
    "  print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply same pre-processing and batching on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = tfio.IODataset.from_mnist(\n",
    "    test_images,\n",
    "    test_labels,\n",
    ")\n",
    "\n",
    "# Shuffle the elements of the dataset.\n",
    "d_test = d_test.shuffle(buffer_size=1024)\n",
    "\n",
    "# By default image data is uint8, so convert to float32 using map().\n",
    "d_test = d_test.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
    "\n",
    "# prepare batches the data just like any other tf.data.Dataset\n",
    "d_test = d_test.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model on both test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(d_train, verbose=0)\n",
    "_, test_acc = model.evaluate(d_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "iterator = iter(d_test)\n",
    "elem = iterator.get_next()[0][0]\n",
    "plt.imshow(elem)\n",
    "prediction = model.predict(np.array([elem]))\n",
    "result = np.where(prediction[0] == np.amax(prediction[0]))\n",
    "print(\"Predicted Value is\" ,result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dmg -i pool destroy -f TEST_POOL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 DAOS TF Environment",
   "language": "python",
   "name": "python3_my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
