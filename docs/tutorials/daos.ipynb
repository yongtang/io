{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f9e30da",
      "metadata": {
        "id": "7857033a12ad"
      },
      "source": [
        "# DAOS Filesystem with Tensorflow (Using MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b37505",
      "metadata": {
        "id": "e5708f933d28"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to use read and write files on [DAOS Filesystem](https://docs.daos.io/) with TensorFlow, through TensorFlow IO's DAOS file system integration.\n",
        "\n",
        "A machine running DAOS natively or through a [docker emulator](https://github.com/daos-stack/daos/tree/master/utils/docker) is needed to run this tutorial and/or use the Tensorflow IO DAOS integration. The DAOS Pool and Container used for this tutorial will be created and deleted within this tutorial, where we will be training and testing a simple Neural Network on the MNIST Dataset loaded from the DAOS File System Plugin.\n",
        "\n",
        "The pool and container id or label are part of the filename uri:\n",
        "```\n",
        "dfs://<pool_id>/<cont_id>/<path>\n",
        "dfs://<pool-label>/cont-label/<path>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3ef6a9",
      "metadata": {
        "id": "b27be7087d0e"
      },
      "source": [
        "## Setup and usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad41a1a",
      "metadata": {
        "id": "e20c1d316af6"
      },
      "source": [
        "### Install required packages, and restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e35916b",
      "metadata": {
        "id": "5de1951509cb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x \n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "!pip install tensorflow-io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7de300",
      "metadata": {
        "id": "d5e736c41c99"
      },
      "source": [
        "### Create Pool and Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79528fed",
      "metadata": {
        "id": "fb83b02da201"
      },
      "outputs": [],
      "source": [
        "!dmg -i pool create -s 500M TEST_POOL\n",
        "!daos cont create --pool=TEST_POOL --type=POSIX TEST_CONT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9e03445ca2b"
      },
      "source": [
        "Importing the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d707f548ed"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5707958e9b2"
      },
      "source": [
        "Initializing our dfs path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b3d02b4bdce"
      },
      "outputs": [],
      "source": [
        "dfs_url = \"dfs://TEST_POOL/TEST_CONT/\" # This the path we'll be using to load and access the dataset\n",
        "pwd = !pwd\n",
        "posix_url = pwd[0] + \"/tests/test_dfs/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb041488c5cc"
      },
      "source": [
        "Install Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d67936c99b0"
      },
      "outputs": [],
      "source": [
        "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/train.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/train_labels.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O $(pwd)/tests/test_dfs/test.gz\n",
        "!wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O $(pwd)/tests/test_dfs/test_labels.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40e9a30808c"
      },
      "source": [
        "Copying the Data from the POSIX Filesystem to the DAOS Filesystem under the pool and container we just created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7a9cb50149f"
      },
      "outputs": [],
      "source": [
        "file_names = [\"train.gz\", \"test.gz\", \"train_labels.gz\", \"test_labels.gz\"]\n",
        "for file in file_names:\n",
        "  tf.io.gfile.copy(posix_url + file, dfs_url + file, True)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1879a8438874"
      },
      "source": [
        "Checking Our Training Images and Training Labels Exist under the specified pool and container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4831a44b46c4"
      },
      "outputs": [],
      "source": [
        "images = dfs_url + \"train.gz\"\n",
        "labels = dfs_url + \"train_labels.gz\"\n",
        "if tf.io.gfile.exists(images) and tf.io.gfile.exists(labels):\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7453b98fc59"
      },
      "source": [
        "Loading MNIST Data from the DFS using tensorflow-io's built in MNIST loading functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b904e495cf5"
      },
      "outputs": [],
      "source": [
        "d_train = tfio.IODataset.from_mnist(\n",
        "    images,\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43e0588f94e4"
      },
      "source": [
        "Pre-processing and Building a simple Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "250109055744"
      },
      "outputs": [],
      "source": [
        "# Shuffle the elements of the dataset.\n",
        "d_train = d_train.shuffle(buffer_size=1024)\n",
        "\n",
        "# By default image data is uint8, so convert to float32 using map().\n",
        "d_train = d_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
        "\n",
        "# prepare batches the data just like any other tf.data.Dataset\n",
        "d_train = d_train.batch(32)\n",
        "\n",
        "# Build the model.\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4129e8e2c1b4"
      },
      "source": [
        "Compiling the model we just built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c9302dea1da"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6193df954a7c"
      },
      "source": [
        "And finally, training on the dataset for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "629b3388da02"
      },
      "outputs": [],
      "source": [
        "history = model.fit(d_train, epochs=15, steps_per_epoch=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb6caec04ac"
      },
      "source": [
        "Plot of Loss vs Epoch during Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14d751d25850"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdcea14afec8"
      },
      "source": [
        "Check Test Data is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c235960fd75"
      },
      "outputs": [],
      "source": [
        "test_images = dfs_url + \"test.gz\"\n",
        "test_labels = dfs_url + \"test_labels.gz\"\n",
        "if tf.io.gfile.exists(test_images) and tf.io.gfile.exists(test_labels):\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bcaff8a9330"
      },
      "source": [
        "Apply same pre-processing and batching on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecd03ced1c6a"
      },
      "outputs": [],
      "source": [
        "d_test = tfio.IODataset.from_mnist(\n",
        "    test_images,\n",
        "    test_labels,\n",
        ")\n",
        "\n",
        "# Shuffle the elements of the dataset.\n",
        "d_test = d_test.shuffle(buffer_size=1024)\n",
        "\n",
        "# By default image data is uint8, so convert to float32 using map().\n",
        "d_test = d_test.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
        "\n",
        "# prepare batches the data just like any other tf.data.Dataset\n",
        "d_test = d_test.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4684eb989dd"
      },
      "source": [
        "Evaluate our model on both test and train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de7d13fa71ef"
      },
      "outputs": [],
      "source": [
        "_, train_acc = model.evaluate(d_train, verbose=0)\n",
        "_, test_acc = model.evaluate(d_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a006d7d2fcb"
      },
      "source": [
        "Prediction Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "701b75aea6c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "iterator = iter(d_test)\n",
        "elem = iterator.get_next()[0][0]\n",
        "plt.imshow(elem)\n",
        "prediction = model.predict(np.array([elem]))\n",
        "result = np.where(prediction[0] == np.amax(prediction[0]))\n",
        "print(\"Predicted Value is\" ,result[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21b12f92e0a2"
      },
      "source": [
        "### Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d847aaa57005"
      },
      "outputs": [],
      "source": [
        "!dmg -i pool destroy -f TEST_POOL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "daos.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
